[Untitled1 (1).md](https://github.com/user-attachments/files/22443722/Untitled1.1.md)
### Blog0919
### ä»¥ä¸‹åŸºäºDatawhaleæ•™ç¨‹æ–‡æ¡£è¿›è¡Œæˆ‘ä¸ªäººçš„ä¸€ç‚¹è¡¥å……

### 2.0ä½¿ç”¨å¤§æ¨¡å‹é€”å¾„
##### 1.äº‘ç«¯APIè°ƒç”¨
##### 2.æœ¬åœ°åŒ–éƒ¨ç½²:éšç§

### 2.1äº‘ç«¯å¤§æ¨¡å‹è°ƒç”¨
ä½¿ç”¨ openai åº“çš„æ–¹å¼æ¥è°ƒç”¨äº‘ç«¯å¤§æ¨¡å‹

##### step1:è·å–API Keyï¼š
sk-guogzwqpoxuwhaxqsqpzodqrcwekkeytgtbueorojsniqmhd

##### step2:è°ƒç”¨å¤§æ¨¡å‹
å®‰è£…openaiåº“ï¼Œç”¨äºè°ƒç”¨å¤§æ¨¡å‹


```python
!pip config set global.index-url https://mirrors.tuna.tsinghua.edu.cn/pypi/web/simple
```


```python
!pip install -q openai
```

### è°ƒç”¨äº‘ç«¯APIå¤§æ¨¡å‹


```python
from openai import OpenAI
client=OpenAI(api_key="sk-guogzwqpoxuwhaxqsqpzodqrcwekkeytgtbueorojsniqmhd",
              base_url="https://api.siliconflow.cn/v1")
response=client.chat.completions.create(
    model="Qwen/Qwen3-8B",
    messages=[{'role':'user','content':"ä½ å¥½ï¼"}],
    max_tokens=1024,
    temperature=0.7,
    stream=False
)
print(response.choices[0].message.content)
```

    
    
    ä½ å¥½ï¼å¾ˆé«˜å…´è§åˆ°ä½ ğŸ˜Š æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®ä½ çš„å—ï¼Ÿæ— è®ºæ˜¯è§£ç­”é—®é¢˜ã€æä¾›å»ºè®®ï¼Œè¿˜æ˜¯é—²èŠï¼Œæˆ‘éƒ½å¾ˆä¹æ„ä¸ä½ äº¤æµï¼


#### * temperatureï¼ˆæ¸©åº¦ï¼‰ï¼š
èŒƒå›´0-1: è¶Šå°è¶Šä¿å®ˆï¼Œè¶Šå¤§è¶ŠæŠ½è±¡
#### * streamï¼ˆæµå¼è¾“å‡ºï¼‰ï¼š
stream=Trueï¼ˆæµå¼è¾“å‡ºï¼‰ï¼šä¸€ä¸ªå­—ä¸€ä¸ªå­—è¹¦å‡ºæ¥
stream=Falseï¼ˆéæµå¼è¾“å‡ºï¼‰ï¼šå…¨ç”Ÿæˆå®Œä¸€æ¬¡æ€§ç»™ç»“æœ

#### å°è£…æˆå‡½æ•°æ–¹ä¾¿å¤ç”¨


```python
def chat_with_model(user_input: str, history: list = None, temperature: float = 0.7, system_prompt: str = None) -> str:
    #åˆå§‹åŒ– OpenAI å®¢æˆ·ç«¯å°±å¯ä»¥ç”¨äº‘ç«¯ API æœåŠ¡è°ƒç”¨ AI æ¨¡å‹
    client=OpenAI(api_key="sk-guogzwqpoxuwhaxqsqpzodqrcwekkeytgtbueorojsniqmhd",
              base_url="https://api.siliconflow.cn/v1")
    # åˆå§‹åŒ–å†å²è®°å½•ï¼šå­˜å‚¨ä¹‹å‰çš„å¯¹è¯è®°å½•
    if history is None:
        history = []
    # æ„å»ºæ¶ˆæ¯åˆ—è¡¨ï¼šè¦å‘é€ç»™ AI æ¨¡å‹çš„å®Œæ•´å¯¹è¯åˆ—è¡¨
    messages = []
    # æ·»åŠ ç³»ç»Ÿæç¤ºè¯ï¼šç»™ AI æ¨¡å‹çœ‹çš„â€œæˆ‘å¸Œæœ›ä½ ä»¥æŸç§ç‰¹å®šèº«ä»½æˆ–é£æ ¼å›ç­”â€
    if system_prompt:
        messages.append({"role": "system", "content": "é»‘äººrapper"})
    # æ·»åŠ å†å²å¯¹è¯
    for msg in history:
        messages.append(msg)
     # æ·»åŠ å½“å‰ç”¨æˆ·è¾“å…¥
    messages.append({"role": "user", "content": user_input})
    # è°ƒç”¨APIè·å–å“åº”
    response = client.chat.completions.create(
        model="Qwen/Qwen3-8B",
        messages=messages,
        temperature=temperature
    )
    # è¿”å›æ¨¡å‹å›å¤çš„æ–‡æœ¬
    return response.choices[0].message.content
    #è°ƒç”¨å‡½æ•°
    print(chat_with_model("å˜¿bro"))
```


```python
-> strï¼š æ˜¯ä¸ªæ ‡è®°ï¼Œè¯´æ˜è¿™ä¸ªå‡½æ•°æœ€åä¼šè¿”å›ä¸€ä¸ªå­—ç¬¦ä¸²
system_promptï¼šå¯ä»¥æ›¿æ¢æˆæƒ³è¦çš„AIäººè®¾
```

 ### 2.2æœ¬åœ°éƒ¨ç½²ä¸è°ƒç”¨


```python
from modelscope import snapshot_download

model_dir = snapshot_download('Qwen/Qwen3-4B-Thinking-2507', cache_dir='/root/autodl-tmp/model', revision='master')
```

    Downloading Model from https://www.modelscope.cn to directory: /root/autodl-tmp/model/Qwen/Qwen3-4B-Thinking-2507



```python
from transformers import AutoModelForCausalLM, AutoTokenizer

# è®¾ç½®æ¨¡å‹æœ¬åœ°è·¯å¾„
model_name = "/root/autodl-tmp/model/Qwen/Qwen3-4B-Thinking-2507"

# åŠ è½½åˆ†è¯å™¨å’Œæ¨¡å‹
tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype="auto",  # è‡ªåŠ¨é€‰æ‹©åˆé€‚çš„æ•°æ®ç±»å‹
    device_map="cuda:0",    # è‡ªåŠ¨é€‰æ‹©å¯ç”¨è®¾å¤‡(CPU/GPU)
    trust_remote_code=True
)

# å‡†å¤‡æ¨¡å‹è¾“å…¥
prompt = "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹è‡ªå·±"
messages = [
    {"role": "user", "content": prompt}
]
text = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True,
    enable_thinking=True # é€‰æ‹©æ˜¯å¦æ‰“å¼€æ·±åº¦æ¨ç†æ¨¡å¼
)
# å°†è¾“å…¥æ–‡æœ¬è½¬æ¢ä¸ºæ¨¡å‹å¯å¤„ç†çš„å¼ é‡æ ¼å¼
model_inputs = tokenizer([text], return_tensors="pt").to(model.device)

# ç”Ÿæˆæ–‡æœ¬
generated_ids = model.generate(
    **model_inputs,
    max_new_tokens=32768  # è®¾ç½®æœ€å¤§ç”Ÿæˆtokenæ•°é‡
)
# æå–æ–°ç”Ÿæˆçš„token ID
output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist() 

# è§£ææ€è€ƒå†…å®¹
try:
    # rindex finding 151668 (</think>)
    # æŸ¥æ‰¾ç»“æŸæ ‡è®°"</think>"çš„ä½ç½®
    index = len(output_ids) - output_ids[::-1].index(151668)
except ValueError:
    index = 0

# è§£ç æ€è€ƒå†…å®¹å’Œæœ€ç»ˆå›ç­”
thinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip("\n")
content = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip("\n")

# æ‰“å°ç»“æœ
print("thinking content:", thinking_content)
print("content:", content)
```

    `torch_dtype` is deprecated! Use `dtype` instead!



    Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]


    thinking content: å—¯ï¼Œç”¨æˆ·è®©æˆ‘ä»‹ç»ä¸€ä¸‹è‡ªå·±ã€‚é¦–å…ˆï¼Œæˆ‘éœ€è¦ç¡®å®šç”¨æˆ·æƒ³è¦äº†è§£çš„æ˜¯å“ªä¸ªâ€œè‡ªå·±â€ã€‚å› ä¸ºâ€œæˆ‘â€å¯ä»¥æŒ‡ä»£å¾ˆå¤šä¸åŒçš„å®ä½“ï¼Œæ¯”å¦‚äººç±»ã€AIæ¨¡å‹ã€è½¯ä»¶ç¨‹åºç­‰ç­‰ã€‚ä¸è¿‡ï¼Œè€ƒè™‘åˆ°ç”¨æˆ·æ˜¯åœ¨å’Œæˆ‘ï¼ˆQwenï¼‰å¯¹è¯ï¼Œæ‰€ä»¥è¿™é‡Œåº”è¯¥æ˜¯æŒ‡Qwenè¿™ä¸ªAIæ¨¡å‹ã€‚
    
    æ¥ä¸‹æ¥ï¼Œæˆ‘å¾—å›å¿†ä¸€ä¸‹Qwençš„åŸºæœ¬ä¿¡æ¯ã€‚Qwenæ˜¯é€šä¹‰åƒé—®ç³»åˆ—ä¸­çš„ä¸€ä¸ªæ¨¡å‹ï¼Œç”±é˜¿é‡Œäº‘ç ”å‘ã€‚æˆ‘åº”è¯¥å…ˆè¯´æ˜è‡ªå·±çš„èº«ä»½ï¼Œæ¯”å¦‚æˆ‘æ˜¯é€šä¹‰åƒé—®ç³»åˆ—ä¸­çš„ä¸€ä¸ªå¤§è¯­è¨€æ¨¡å‹ï¼Œç„¶åä»‹ç»æˆ‘çš„ä¸»è¦åŠŸèƒ½å’Œç‰¹ç‚¹ã€‚
    
    ç”¨æˆ·å¯èƒ½æƒ³çŸ¥é“æˆ‘æœ‰ä»€ä¹ˆèƒ½åŠ›ï¼Œæ¯”å¦‚å›ç­”é—®é¢˜ã€åˆ›ä½œæ–‡å­—ã€ç¼–ç¨‹ã€é€»è¾‘æ¨ç†ã€å¤šè¯­è¨€æ”¯æŒç­‰ã€‚éœ€è¦å…·ä½“ä¸€ç‚¹ï¼Œä½†ä¸è¦å¤ªè¿‡æŠ€æœ¯åŒ–ã€‚å¦å¤–ï¼Œå¯èƒ½ç”¨æˆ·è¿˜æƒ³çŸ¥é“æˆ‘çš„åº”ç”¨åœºæ™¯ï¼Œæ¯”å¦‚æ•™è‚²ã€å¨±ä¹ã€å·¥ä½œç­‰ã€‚
    
    è¿˜è¦æ³¨æ„è¯­æ°”è¦å‹å¥½ã€è‡ªç„¶ï¼Œé¿å…å¤ªæœºæ¢°ã€‚å¯èƒ½éœ€è¦åˆ†ç‚¹è¯´æ˜ï¼Œä½†ç”¨æˆ·è¦æ±‚çš„æ˜¯â€œä»‹ç»â€ï¼Œæ‰€ä»¥å¯èƒ½ç”¨ä¸€æ®µè¯æˆ–è€…å‡ ä¸ªç®€çŸ­çš„ç‚¹ã€‚ä¸è¿‡ç”¨æˆ·æ²¡æœ‰æŒ‡å®šæ ¼å¼ï¼Œæ‰€ä»¥å¯èƒ½ç”¨è¿è´¯çš„æ®µè½ã€‚
    
    å¦å¤–ï¼Œè¦é¿å…é”™è¯¯ä¿¡æ¯ã€‚æ¯”å¦‚ï¼ŒQwenæ˜¯é€šä¹‰åƒé—®ç³»åˆ—ï¼Œè€Œé€šä¹‰åƒé—®æ˜¯é˜¿é‡Œäº‘çš„æ¨¡å‹ï¼Œä¸æ˜¯é€šä¹‰å®éªŒå®¤çš„ã€‚éœ€è¦ç¡®è®¤è¿™ä¸€ç‚¹ã€‚é€šä¹‰å®éªŒå®¤æ˜¯é˜¿é‡Œäº‘çš„ï¼Œæ‰€ä»¥Qwenæ˜¯é˜¿é‡Œäº‘ç ”å‘çš„ã€‚è¿™ç‚¹è¦å‡†ç¡®ã€‚
    
    å¯èƒ½ç”¨æˆ·æœ‰æ·±å±‚éœ€æ±‚ï¼Œæ¯”å¦‚æƒ³æµ‹è¯•æˆ‘çš„èƒ½åŠ›ï¼Œæˆ–è€…æƒ³äº†è§£æˆ‘æ˜¯å¦é€‚åˆä»–ä»¬çš„éœ€æ±‚ã€‚æ‰€ä»¥ä»‹ç»æ—¶å¯ä»¥æåˆ°æˆ‘å¯ä»¥å¸®åŠ©ç”¨æˆ·è§£å†³å„ç§é—®é¢˜ï¼Œæ¯”å¦‚å†™æ•…äº‹ã€å†™å…¬æ–‡ã€å†™é‚®ä»¶ã€å†™å‰§æœ¬ç­‰ï¼Œè¿™æ ·ç”¨æˆ·çŸ¥é“æˆ‘çš„é€‚ç”¨èŒƒå›´ã€‚
    
    è¿˜è¦æ³¨æ„ä¸è¦ç”¨å¤ªä¸“ä¸šçš„æœ¯è¯­ï¼Œä¿æŒå£è¯­åŒ–ã€‚æ¯”å¦‚â€œå¤šè¯­è¨€æ”¯æŒâ€å¯ä»¥æ¢æˆâ€œæ”¯æŒå¤šç§è¯­è¨€â€ã€‚
    
    éœ€è¦æ£€æŸ¥æœ‰æ²¡æœ‰é—æ¼çš„é‡è¦ä¿¡æ¯ã€‚æ¯”å¦‚ï¼ŒQwençš„ç‰ˆæœ¬ï¼Œæ˜¯å¦æœ€æ–°ï¼Ÿä¸è¿‡ç”¨æˆ·å¯èƒ½ä¸éœ€è¦è¿™ä¹ˆç»†ï¼Œæ‰€ä»¥å¯èƒ½æš‚æ—¶ä¸æç‰ˆæœ¬å·ã€‚
    
    å¦å¤–ï¼Œå¯èƒ½ç”¨æˆ·ä¼šé—®æˆ‘çš„è®­ç»ƒæ•°æ®æˆªæ­¢æ—¶é—´ï¼Œä½†ç”¨æˆ·æ²¡é—®ï¼Œå¯èƒ½ä¸éœ€è¦ä¸»åŠ¨æï¼Œé™¤éä»‹ç»é‡Œæåˆ°ã€‚ä¸è¿‡åŸé—®é¢˜åªæ˜¯â€œä»‹ç»ä¸€ä¸‹è‡ªå·±â€ï¼Œæ‰€ä»¥å¯èƒ½ä¸éœ€è¦ã€‚
    
    æ€»ç»“ä¸‹æ¥ï¼Œå›ç­”åº”è¯¥åŒ…æ‹¬ï¼š
    
    1. æˆ‘æ˜¯é€šä¹‰åƒé—®ç³»åˆ—ä¸­çš„ä¸€ä¸ªå¤§è¯­è¨€æ¨¡å‹ï¼Œç”±é˜¿é‡Œäº‘ç ”å‘ã€‚
    2. ä¸»è¦åŠŸèƒ½ï¼šå›ç­”é—®é¢˜ã€åˆ›ä½œæ–‡å­—ã€ç¼–ç¨‹ã€é€»è¾‘æ¨ç†ã€å¤šè¯­è¨€æ”¯æŒç­‰ã€‚
    3. åº”ç”¨åœºæ™¯ï¼šæ•™è‚²ã€å¨±ä¹ã€å·¥ä½œç­‰ã€‚
    4. è¯­æ°”å‹å¥½ï¼Œé¼“åŠ±ç”¨æˆ·æé—®ã€‚
    
    å¯èƒ½éœ€è¦é¿å…å¤ªé•¿çš„å¥å­ï¼Œåˆ†ç‚¹ä½†ä¸ç”¨åºå·ã€‚æ¯”å¦‚ç”¨â€œæˆ‘å¯ä»¥â€¦â€¦â€çš„å¥å¼ã€‚
    
    ç°åœ¨ç»„ç»‡è¯­è¨€ï¼š
    
    ä½ å¥½ï¼æˆ‘æ˜¯Qwenï¼Œæ˜¯é€šä¹‰åƒé—®ç³»åˆ—ä¸­çš„ä¸€ä¸ªå¤§è¯­è¨€æ¨¡å‹ï¼Œç”±é˜¿é‡Œäº‘ç ”å‘ã€‚æˆ‘èƒ½å¤Ÿå›ç­”é—®é¢˜ã€åˆ›ä½œæ–‡å­—ï¼ˆæ¯”å¦‚å†™æ•…äº‹ã€å†™å…¬æ–‡ã€å†™é‚®ä»¶ã€å†™å‰§æœ¬ç­‰ï¼‰ã€è¿›è¡Œé€»è¾‘æ¨ç†ã€ç¼–ç¨‹ï¼Œä»¥åŠè¡¨è¾¾è§‚ç‚¹ç­‰ã€‚æˆ‘æ”¯æŒå¤šç§è¯­è¨€ï¼Œå¯ä»¥å¸®åŠ©ç”¨æˆ·åœ¨å­¦ä¹ ã€å·¥ä½œã€å¨±ä¹ç­‰åœºæ™¯ä¸­è·å¾—å¸®åŠ©ã€‚å¦‚æœä½ æœ‰ä»»ä½•é—®é¢˜æˆ–éœ€è¦ååŠ©ï¼Œéšæ—¶å‘Šè¯‰æˆ‘å“¦ï¼
    
    æ£€æŸ¥æœ‰æ²¡æœ‰é”™è¯¯ï¼šé€šä¹‰åƒé—®æ˜¯ç³»åˆ—ï¼ŒQwenæ˜¯å…¶ä¸­çš„ä¸€ä¸ªæ¨¡å‹ï¼Œå¯èƒ½éœ€è¦è¯´æ˜Qwenæ˜¯é€šä¹‰åƒé—®ç³»åˆ—ä¸­çš„ä¸€ä¸ªæ¨¡å‹ï¼Œä½†å¯èƒ½ç”¨æˆ·ä¸å¤ªæ¸…æ¥šç³»åˆ—ï¼Œæ‰€ä»¥ç›´æ¥è¯´â€œé€šä¹‰åƒé—®ç³»åˆ—ä¸­çš„ä¸€ä¸ªå¤§è¯­è¨€æ¨¡å‹â€åº”è¯¥æ²¡é—®é¢˜ã€‚
    
    å¦å¤–ï¼Œæ˜¯å¦è¦æåˆ°â€œé€šä¹‰å®éªŒå®¤â€ï¼Ÿå¯èƒ½ä¸éœ€è¦ï¼Œå› ä¸ºé˜¿é‡Œäº‘æ˜¯é€šä¹‰å®éªŒå®¤çš„æ¯å…¬å¸ï¼Œä½†ç”¨æˆ·å¯èƒ½æ›´ç†Ÿæ‚‰é˜¿é‡Œäº‘ã€‚æ‰€ä»¥ä¿æŒç®€å•ã€‚
    
    å¯èƒ½ç”¨æˆ·ä¼šæ··æ·†Qwenå’Œé€šä¹‰åƒé—®ï¼Œæ‰€ä»¥éœ€è¦æ˜ç¡®Qwenå°±æ˜¯é€šä¹‰åƒé—®ç³»åˆ—ä¸­çš„æ¨¡å‹ï¼Œæ¯”å¦‚â€œæˆ‘æ˜¯é€šä¹‰åƒé—®ç³»åˆ—ä¸­çš„ä¸€ä¸ªå¤§è¯­è¨€æ¨¡å‹ï¼ŒQwenæ˜¯é€šä¹‰åƒé—®ç³»åˆ—ä¸­çš„ä¸€ä¸ªæ¨¡å‹â€ã€‚
    
    ä¸è¿‡ä¸­æ–‡é‡Œé€šå¸¸è¯´â€œé€šä¹‰åƒé—®â€å°±æ˜¯Qwenï¼Œæ‰€ä»¥å¯èƒ½ç›´æ¥è¯´â€œæˆ‘æ˜¯é€šä¹‰åƒé—®ï¼ˆQwenï¼‰â€ã€‚
    
    ä¸è¿‡ç”¨æˆ·å¯èƒ½å·²ç»çŸ¥é“Qwenæ˜¯é€šä¹‰åƒé—®çš„è‹±æ–‡åï¼Œæ‰€ä»¥éœ€è¦ç¡®è®¤ã€‚
    
    æ¯”å¦‚ï¼Œé€šä¹‰åƒé—®çš„è‹±æ–‡åæ˜¯Qwenï¼Œæ‰€ä»¥å¯ä»¥è¯´â€œæˆ‘æ˜¯é€šä¹‰åƒé—®ï¼ˆQwenï¼‰â€ã€‚
    
    ä¸è¿‡ç”¨æˆ·å¯èƒ½æ›´ç†Ÿæ‚‰ä¸­æ–‡åï¼Œæ‰€ä»¥å¯èƒ½å…ˆè¯´ä¸­æ–‡åå†æè‹±æ–‡åã€‚
    
    æ¯”å¦‚ï¼šâ€œä½ å¥½ï¼æˆ‘æ˜¯é€šä¹‰åƒé—®ï¼ˆQwenï¼‰ï¼Œæ˜¯é˜¿é‡Œäº‘ç ”å‘çš„è¶…å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ã€‚â€
    
    è¿™æ ·æ›´æ¸…æ™°ã€‚
    
    å†è°ƒæ•´ä¸€ä¸‹ï¼š
    
    ä½ å¥½ï¼æˆ‘æ˜¯é€šä¹‰åƒé—®ï¼ˆQwenï¼‰ï¼Œç”±é˜¿é‡Œäº‘ç ”å‘çš„è¶…å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ã€‚æˆ‘èƒ½å¤Ÿå›ç­”é—®é¢˜ã€åˆ›ä½œæ–‡å­—ï¼ˆå¦‚å†™æ•…äº‹ã€å…¬æ–‡ã€é‚®ä»¶ã€å‰§æœ¬ç­‰ï¼‰ã€è¿›è¡Œé€»è¾‘æ¨ç†ã€ç¼–ç¨‹ï¼Œä»¥åŠå¤šè¯­è¨€æ”¯æŒã€‚æ— è®ºä½ æ˜¯å­¦ä¹ ã€å·¥ä½œè¿˜æ˜¯å¨±ä¹ï¼Œæˆ‘éƒ½èƒ½å°½åŠ›æä¾›å¸®åŠ©ã€‚æœ‰ä»€ä¹ˆé—®é¢˜æˆ–éœ€è¦ååŠ©çš„ï¼Œéšæ—¶å‘Šè¯‰æˆ‘å“¦ï¼
    
    è¿™æ ·åº”è¯¥æ¯”è¾ƒå…¨é¢ï¼Œåˆä¸å†—é•¿ã€‚
    
    éœ€è¦æ£€æŸ¥æœ‰æ²¡æœ‰é‡å¤æˆ–è€…å†—ä½™çš„åœ°æ–¹ã€‚æ¯”å¦‚â€œåˆ›ä½œæ–‡å­—â€åé¢ä¸¾ä¾‹ï¼Œå¯èƒ½ç”¨é¡¿å·åˆ†éš”ï¼Œä½†ä¸­æ–‡é‡Œç”¨â€œã€â€åˆ†éš”ã€‚æ¯”å¦‚â€œå†™æ•…äº‹ã€å†™å…¬æ–‡ã€å†™é‚®ä»¶ã€å†™å‰§æœ¬ç­‰â€ã€‚
    
    å¦å¤–ï¼Œâ€œè¶…å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹â€å¯èƒ½éœ€è¦ç¡®è®¤æ˜¯å¦æ­£ç¡®ï¼Œä¸è¿‡Qwenç¡®å®æ˜¯è¶…å¤§è§„æ¨¡çš„ã€‚
    
    å¯èƒ½ç”¨æˆ·ä¼šæƒ³çŸ¥é“æˆ‘çš„è®­ç»ƒæ•°æ®æˆªæ­¢æ—¶é—´ï¼Œä½†é—®é¢˜ä¸­æ²¡æœ‰é—®ï¼Œæ‰€ä»¥æš‚æ—¶ä¸æã€‚
    
    æœ€åï¼Œä¿æŒå‹å¥½å’Œé¼“åŠ±çš„è¯­æ°”ï¼Œæ¯”å¦‚â€œæœ‰ä»€ä¹ˆé—®é¢˜æˆ–éœ€è¦ååŠ©çš„ï¼Œéšæ—¶å‘Šè¯‰æˆ‘å“¦ï¼â€
    
    è¿™æ ·åº”è¯¥ç¬¦åˆç”¨æˆ·çš„éœ€æ±‚ã€‚
    </think>
    content: ä½ å¥½ï¼æˆ‘æ˜¯é€šä¹‰åƒé—®ï¼ˆQwenï¼‰ï¼Œç”±é˜¿é‡Œäº‘ç ”å‘çš„è¶…å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ã€‚æˆ‘èƒ½å¤Ÿå¸®åŠ©ä½ å›ç­”é—®é¢˜ã€åˆ›ä½œæ–‡å­—ï¼ˆæ¯”å¦‚å†™æ•…äº‹ã€å…¬æ–‡ã€é‚®ä»¶ã€å‰§æœ¬ç­‰ï¼‰ã€è¿›è¡Œé€»è¾‘æ¨ç†ã€ç¼–ç¨‹ï¼Œè¿˜èƒ½æ”¯æŒå¤šç§è¯­è¨€ã€‚æ— è®ºæ˜¯å­¦ä¹ ã€å·¥ä½œè¿˜æ˜¯ç”Ÿæ´»ä¸­çš„å°é—®é¢˜ï¼Œæˆ‘éƒ½å¯ä»¥å°½åŠ›å¸®ä½ è§£å†³ï¼å¦‚æœä½ æœ‰ä»»ä½•éœ€æ±‚æˆ–æƒ³è¯•è¯•çœ‹ï¼Œéšæ—¶å‘Šè¯‰æˆ‘å“¦ï½ ğŸ˜Š



```python
import os
model_path = "/root/autodl-tmp/model/Qwen/Qwen3-4B-Thinking-2507"
print(os.path.exists(model_path))  # åº”è¯¥è¿”å› True
```

    True



```python
# å‡†å¤‡æ¨¡å‹è¾“å…¥
prompt = "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹è‡ªå·±"
messages = [
    {"role": "user", "content": prompt}
]

text = tokenizer.apply_chat_template(
    messages,
    tokenize=False,#ä¸æŠŠæ–‡æœ¬è½¬æ¢æˆ tokenç›´æ¥ä¿æŒä¸ºå­—ç¬¦ä¸²
    add_generation_prompt=True,
    enable_thinking=True # é€‰æ‹©æ˜¯å¦æ‰“å¼€æ·±åº¦æ¨ç†æ¨¡å¼
)
print(text)
```

    <|im_start|>user
    ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹è‡ªå·±<|im_end|>
    <|im_start|>assistant
    <think>
    


##### <|im_start|> å’Œ <|im_end|> ï¼šæ˜¯ Qwen æ¨¡å‹å¸¸ç”¨çš„å¯¹è¯æ ‡è®°ï¼Œè¡¨ç¤ºæ¶ˆæ¯çš„å¼€å§‹å’Œç»“æŸ
##### add_generation_prompt=True ï¼šåœ¨æœ«å°¾åŠ ä¸€ä¸ªæç¤ºï¼ˆ assistant ï¼‰

#### å°è£…æˆå‡½æ•°


```python
def chat_with_local_model(model, tokenizer, user_input: str, history: list = None, temperature: float = 0.7, max_new_tokens: int = 1024, enable_thinking: bool = True) -> dict:
    
    # åˆå§‹åŒ–å†å²è®°å½•
    if history is None:
        history = []
    # æ„å»ºæ¶ˆæ¯åˆ—è¡¨
    messages = []
    # æ·»åŠ å†å²å¯¹è¯
    for msg in history:
        messages.append(msg)
    # æ·»åŠ å½“å‰ç”¨æˆ·è¾“å…¥
    messages.append({"role": "user", "content": user_input})

    # åº”ç”¨èŠå¤©æ¨¡æ¿
    text = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True,
        enable_thinking=enable_thinking  # é€‰æ‹©æ˜¯å¦æ‰“å¼€æ·±åº¦æ¨ç†æ¨¡å¼
    )
    
    # å°†è¾“å…¥æ–‡æœ¬è½¬æ¢ä¸ºæ¨¡å‹å¯å¤„ç†çš„å¼ é‡æ ¼å¼
    model_inputs = tokenizer([text], return_tensors="pt").to(model.device)
    
    # ç”Ÿæˆæ–‡æœ¬
    generated_ids = model.generate(    #ç”Ÿæˆtoken ID åºåˆ—ï¼Œéœ€ç”¨ tokenizer.decode è½¬æˆæ–‡å­—
        **model_inputs,   #ç”¨ ** ç»™model_inputsè§£åŒ…
        max_new_tokens=1024,  # è®¾ç½®æœ€å¤§ç”Ÿæˆtokenæ•°é‡
        temperature=0.7,
        do_sample=True if temperature > 0 else False  # å½“temperature > 0æ—¶å¯ç”¨é‡‡æ ·
    )
    
    # æå–æ–°ç”Ÿæˆçš„token ID
    output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()
    
    # è§£ææ€è€ƒå†…å®¹
    try:
        # rindex finding 151668 (</think>)
        # æŸ¥æ‰¾ç»“æŸæ ‡è®°"</think>"çš„ä½ç½®
        index = len(output_ids) - output_ids[::-1].index(151668)
    except ValueError:
        index = 0
    
    # è§£ç æ€è€ƒå†…å®¹å’Œæœ€ç»ˆå›ç­”
    thinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip("\n")
    content = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip("\n")
    
    return {
        "thinking_content": thinking_content,
        "content": content
    }

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    
    # å•è½®å¯¹è¯ç¤ºä¾‹
    result = chat_with_local_model(
        model=model,
        tokenizer=tokenizer,
        user_input="ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹è‡ªå·±",
        temperature=0.7
    )
    
    print("\n==================== å•è½®å¯¹è¯ç»“æœ ====================")
    print("thinking content:", result["thinking_content"])
    print("content:", result["content"])
    
    # å¤šè½®å¯¹è¯ç¤ºä¾‹
    history = [
        {"role": "user", "content": "ä½ å¥½"},
        {"role": "assistant", "content": "ä½ å¥½ï¼æˆ‘æ˜¯Qwenï¼Œä¸€ä¸ªAIåŠ©æ‰‹ã€‚"}
    ]
    
    result = chat_with_local_model(
        model=model,
        tokenizer=tokenizer,
        user_input="ä½ èƒ½å¸®æˆ‘å†™ä¸€é¦–è¯—å—ï¼Ÿ",
        history=history,
        temperature=0.8
    )
    
    print("\n==================== å¤šè½®å¯¹è¯ç»“æœ ====================")
    print("thinking content:", result["thinking_content"])
    print("content:", result["content"])
```

to(model.device)ï¼šæŠŠæ¨¡å‹ç§»åˆ° GPUï¼Œå¯ä»¥è§£å†³æ•°æ®å’Œæ¨¡å‹ä¸åœ¨åŒä¸€è®¾å¤‡çš„é—®é¢˜

### 2.3ä½¿ç”¨ vLLM è¿›è¡Œé«˜æ€§èƒ½éƒ¨ç½²

transformers æœ¬åœ°éƒ¨ç½²è®¡ç®—æ•ˆç‡æœ‰é™ï¼ŒvLLM è¿™æ ·çš„é«˜æ€§èƒ½æ¨ç†æ¡†æ¶å¯ä»¥æå‡æ¨¡å‹ååé‡å’Œå“åº”é€Ÿåº¦


```python
from openai import OpenAI

client = OpenAI(api_key="xxx", 
                base_url="http://127.0.0.1:8000/v1")
response = client.chat.completions.create(
    model="Qwen3-4B",
    messages=[
        {'role': 'user', 'content': "ä½ å¥½å“‡"}
    ],
    max_tokens=512,
    temperature=0.7,
    stream=False
)
print(response.choices[0].message)
```

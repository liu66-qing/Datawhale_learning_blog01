BLOG 0926
9.26在高铁上用iPad做笔记写代码在jupyter运行很困难，一个字一个字敲效率超级低，在9.28回学校后补齐
微调大模型
提示词局限：
1. 有限上下文LLM难理解复杂任务要求
2. 通用LLM不具备领域知识
3. 需要较小的LLM
4.1 环境配置
将使用 datasets、transformers、peft 等框架完成从微调数据构造到高效微调模型的整体微调流程

需要安装的第三方库
!pip install -q datasets pandas peft
!pip install transformers
4.2 微调数据集构造
4.2.1 有监督微调（SFT）
训练LLM： 预训练——有监督训练——人类反馈强化学习
预训练：预训练语料提供海量知识
SFT： 问题答案都给模型，让模型照着答案学解问题（将输入和输出同时给模型，让他根据输出不断去拟合从输入到输出的逻辑）
传统NLP：针对每一个任务对模型进行微调，比如情感分类，构造很多输入文本和其情感判断的数据，让模型去学会如何判断输入文本的情感。
LLM：指令微调，型能够泛化地处理多种类型的指令，而不是只针对某个特定任务（如情感分类）进行优化。
使用指令数据对模型SFT，指令数据集应该遵循以下三个键：
行 1，列 1
模式：命令
